r"""
``contk.metrics`` provides classes and functions evaluating results of models. It provides
a fair metric for every model.
"""
import random

import numpy as np
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from nltk.translate.bleu_score import sentence_bleu

from .._utils.unordered_hash import UnorderedSha256

class MetricBase:
	'''Base class for metrics.
	'''
	def __init__(self):
		pass

class _PrecisionRecallMetric(MetricBase):
	'''Base class for precision recall metrics. This is an abstract class.

	Arguments:
	    reference_key (str): Reference sentences are passed to :func:`forward` by
	    	``data[reference_key]``. Default: ``resp``.
		gen_key (str): Sentences generated by model are passed to :func:.forward by
			``data[gen_key]``. Default: ``gen``.

	Attributes:
	    res_prefix (str): Prefix added to the front of each key
	    				in the result dict of ^close^
	'''
	def __init__(self, reference_key='resp', gen_key='gen'):
		super().__init__()
		self.reference_key = reference_key
		self.gen_key = gen_key
		self.prec_list = []
		self.rec_list = []
		self.res_prefix = ""

	def score(self, gen, reference):
		r'''This function is called by ^forward^

		Arguments:
		    * gen (list): list of generated word ids
		    * reference (list): list of word ids of a reference

        Returns:
            (scalar): score \in [0, 1]
        '''
		raise NotImplementedError( \
			"This function should be implemented by subclasses.")

	def forward(self, data):
		'''Processing a batch of data.

        Arguments:
            data (dict): A dict at least contains the following keys.
            data[reference_key] (list of list of list): Reference sentences.
                Does not contain start token (eg: ``<go>``) and end token (eg: ``<eos>``).
                Outermost list: batch_size
                Innermost list: number of words, allow different sizes
                Second innermost list: number of sentences, allow different sizes
            data[gen_prob_key] (list of list of list): Setence generations model outputs
                similar to data[reference_key]
        '''
		references = data[self.reference_key]
		gens = data[self.gen_key]

		if len(references) != len(gens):
			raise ValueError("Batch num is not matched.")

		for reference, gen in zip(references, gens):
			# pylint: disable=no-member
			matrix = np.zeros((len(reference), len(gen)), dtype=np.float32)
			for i, single_ref in enumerate(reference):
				for j, single_gen in enumerate(gen):
					matrix[i][j] = self.score(single_gen, single_ref)
			self.prec_list.append(float(np.sum(np.max(matrix, 0))) / len(gen))
			self.rec_list.append(float(np.sum(np.max(matrix, 1))) / len(references))

	def close(self):
		'''Return a dict which contains:

			* **precision**: average precision
			* **recall**: average recall
		'''
		return {'{} precision'.format(self.res_prefix): np.average(self.prec_list), \
				'{} recall'.format(self.res_prefix): np.average(self.rec_list)}

class BleuPrecisionRecallMetric(_PrecisionRecallMetric):
	'''Metric for calculating sentence BLEU precision and recall

	Arguments:
	    * ngram (int): Specifies BLEU-ngram
	'''
	def __init__(self, ngram, reference_key='resp', gen_key='gen'):
		super().__init__(reference_key, gen_key)
		if ngram not in range(1, 5):
			raise ValueError("ngram should belong to [1, 4]")
		self.ngram = ngram
		self.weights = [1 / ngram] * ngram
		self.res_prefix = 'BLEU-{}'.format(ngram)

	def score(self, gen, reference):
		r'''Score_fn of BLEU-ngram precision and recall

        Returns:
            (scalar): sentence bleu score \in [0, 1]
        '''
		return sentence_bleu([reference], gen, self.weights, SmoothingFunction().method1)

class EmbSimilarityPrecisionRecallMetric(_PrecisionRecallMetric):
	'''Metric for calculating cosine similarity precision and recall

	Arguments:
		* embed (:class:^numpy.array^): A 2-d padded array of word embeddings
		* mode (str): Specifies the operation that computes the bag-of-word representation.
					Must be 'avg' or 'extrema':
						'avg': element-wise average word embeddings
						'extrema': element-wise maximum word embeddings
	'''
	def __init__(self, embed, mode, reference_key='resp', gen_key='gen'):
		super().__init__(reference_key, gen_key)
		if not isinstance(embed, np.ndarray) or len(np.shape(embed)) != 2:
			raise ValueError("invalid type of shape of embed.")
		if mode not in ['avg', 'extrema']:
			raise ValueError("mode should be 'avg' or 'extrema'.")
		self.embed = embed
		self.mode = mode
		self.res_prefix = '{}-bow'.format(mode)

	def score(self, gen, reference):
		r'''Score_fn of cosine similarity precision and recall

        Returns:
            (Scalar): cosine similarity between two sentence embeddings \in [0, 1]
        '''
		gen_vec = []
		ref_vec = []
		for i in gen:
			if i < 0 or i >= len(self.embed):
				raise ValueError("gen index out of range.")
			gen_vec.append(self.embed[i])
		for i in reference:
			if i < 0 or i >= len(self.embed):
				raise ValueError("reference index out of range.")
			ref_vec.append(self.embed[i])
		if self.mode == 'avg':
			gen_embed = np.average(gen_vec, 0)
			ref_embed = np.average(ref_vec, 0)
		else:
			gen_embed = np.max(gen_vec, 0)
			ref_embed = np.max(ref_vec, 0)
		cos = np.sum(gen_embed * ref_embed) / \
			  np.sqrt(np.sum(gen_embed * gen_embed) * np.sum(ref_embed * ref_embed))
		norm = (cos + 1) / 2
		return norm

class PerlplexityMetric(MetricBase):
	'''Metric for calcualting perplexity.

	Arguments:
		reference_key (str): Reference sentences are passed to :func:`forward` by ``data[reference_key]``.
			Default: ``resp``.
		reference_len_key (str): Length of reference sentences are passed to :func:`forward`
			by ``data[reference_len_key]``. Default: ``resp_length``.
		gen_prob_key (str): Sentence generations model outputs of **log softmax** probability
			are passed to :func:`forward` by ``data[gen_prob_key]``. Default: ``gen_prob``.
	'''
	def __init__(self, dataloader, reference_key="resp", \
					   reference_len_key="resp_length", \
					   gen_prob_key="gen_prob", \
					   full_check=False \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.reference_key = reference_key
		self.reference_len_key = reference_len_key
		self.gen_prob_key = gen_prob_key
		self.word_loss = 0
		self.length_sum = 0
		self.full_check = full_check

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys.
			data[reference_key] (list or :class:`numpy.array`): Reference sentences.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, max_sentence_length]`
			data[reference_len_key] (list): Length of Reference sentences. Contains start token (eg:``<go>``)
				and end token (eg:``<eos>``). Size: `[batch_size]`
			data[gen_prob_key] (list or :class:`numpy.array`): Setence generations model outputs of
				**log softmax** probability. Contains end token (eg:``<eos>``), but without start token
				(eg: ``<go>``).	The 2nd dimension can be jagged.
				Size: `[batch_size, gen_sentence_length, vocab_size]`.

		Warning:
			``data[gen_prob_key]`` must be processed after log_softmax. That means,
			``np.sum(np.exp(gen_prob), -1)`` equals ``np.ones((batch_size, gen_sentence_length))``
		'''
		resp = data[self.reference_key]
		resp_length = data[self.reference_len_key]
		gen_prob = data[self.gen_prob_key]
		if len(resp) != len(resp_length) or len(resp) != len(gen_prob):
			raise ValueError("Batch num is not matched.")

		# perform random check to assert the probability is valid
		checkid = random.randint(0, len(resp_length)-1)
		checkrow = random.randint(0, resp_length[checkid]-2)
		if not np.isclose(np.sum(np.exp(gen_prob[checkid][checkrow])), 1):
			print("gen_prob[%d][%d] exp sum is equal to %f." % (checkid, checkrow, \
				np.sum(np.exp(gen_prob[checkid][checkrow]))))
			raise ValueError("data[gen_prob_key] must be processed after log_softmax.")

		for i, single_length in enumerate(resp_length):
			# perform full check to assert the probability is valid
			if self.full_check:
				expsum = np.sum(np.exp(gen_prob[i][:single_length]), -1)
				if not np.allclose(expsum, [1] * single_length):
					raise ValueError("data[gen_prob_key] must be processed after log_softmax.")

			self.word_loss += -np.sum(np.array(gen_prob[i])[\
				list(range(single_length-1)), resp[i][1:single_length]])
			self.length_sum += single_length - 1

	def close(self):
		'''Return a dict which contains:

			* **perplexity**: perplexity value
		'''
		return {"perplexity": np.exp(self.word_loss / self.length_sum)}

class MultiTurnPerplexityMetric(MetricBase):
	'''Metric for calcualting multi-turn perplexity.

	Arguments:
		reference_key (str): Reference sentences are passed to :func:`forward` by ``data[reference_key]``.
			Default: ``sent``.
		reference_len_key (str): Length of reference sentences are passed to :func:`forward`
			by ``data[reference_len_key]``. Default: ``sent_length``.
		gen_prob_key (str): Sentence generations model outputs of **log softmax** probability
			are passed to :func:`forward` by ``data[gen_prob_key]``. Default: ``gen_prob``.
	'''
	def __init__(self, dataloader, reference_key="sent", \
					   reference_len_key="sent_length", \
					   gen_prob_key="gen_prob", \
					   full_check=False \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.reference_key = reference_key
		self.reference_len_key = reference_len_key
		self.gen_prob_key = gen_prob_key
		self.sub_metric = PerlplexityMetric(dataloader, reference_key="sent", \
				reference_len_key="sent_length", gen_prob_key="gen_prob", full_check=full_check)

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys.
			data[reference_key] (list or :class:`numpy.array`): Reference sentences.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, max_turn_length, max_sentence_length]`
			data[reference_len_key] (list of list): Length of Reference sentences. Contains
				start token (eg:``<go>``) and end token (eg:``<eos>``). It must NOT be padded,
				which means the inner lists may have different length.
				Length of outer list: `batch_size`
			data[gen_prob_key] (list or :class:`numpy.array`): Setence generations model outputs of
				**log softmax** probability. Contains end token (eg:``<eos>``), but without start token
				(eg: ``<go>``).	The 2nd / 3rd dimension can be jagged.
				Size: `[batch_size, max_turn_length, gen_sentence_length, vocab_size]`.

		Warning:
			``data[gen_prob_key]`` must be processed after log_softmax. That means,
			``np.sum(np.exp(gen_prob), -1)`` equals ``np.ones((batch_size, gen_sentence_length))``
		'''
		reference = data[self.reference_key]
		length = data[self.reference_len_key]
		gen_prob = data[self.gen_prob_key]
		if len(length) != len(reference) or len(length) != len(gen_prob):
			raise ValueError("Batch num is not matched.")

		for i, sent_length in enumerate(length):
			# Pass turn as batch for sub_metric, the result will be same.
			turn_length = len(sent_length)
			self.sub_metric.forward({"sent": reference[i][:turn_length], \
					"sent_length": sent_length, \
					"gen_prob": gen_prob[i][:turn_length]})

	def close(self):
		'''Return a dict which contains:

			* **perplexity**: perplexity value
		'''
		return self.sub_metric.close()

class BleuCorpusMetric(MetricBase):
	'''Metric for calcualting BLEU.

	Arguments:
		reference_key (str): Reference sentences are passed to :func:.forward by ``data[reference_key]``.
			Default: ``resp``.
		gen_key (str): Sentences generated by model are passed to :func:.forward by
			``data[gen_prob_key]``. Default: ``gen``.
	'''
	def __init__(self, dataloader, reference_key="resp", gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.reference_key = reference_key
		self.gen_key = gen_key
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys.
			data[reference_key] (list or :class:`numpy.array` of `int`): Reference sentences.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, max_sentence_length]`
			data[gen_key] (list or :class:`numpy.array` of `int`): Setences generated by model.
				Contains end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
				Size: `[batch_size, gen_sentence_length]`.
		'''
		gen = data[self.gen_key]
		resp = data[self.reference_key]
		if len(resp) != len(gen):
			raise ValueError("Batch num is not matched.")

		for gen_sen, resp_sen in zip(gen, resp):
			self.hyps.append(self.dataloader.trim_index(gen_sen))
			self.refs.append([self.dataloader.trim_index(resp_sen[1:])])

	def close(self):
		'''Return a dict which contains:

			* **bleu**: bleu value.
		'''
		try:
			return {"bleu": \
				corpus_bleu(self.refs, self.hyps, smoothing_function=SmoothingFunction().method7)}
		except ZeroDivisionError as _:
			raise ZeroDivisionError("Bleu smoothing divided by zero. This is a known bug of corpus_bleu, \
				usually caused when there is only one sample and the sample length is 1.")

class MultiTurnBleuCorpusMetric(MetricBase):
	'''Metric for calcualting multi-turn BLEU.

	Arguments:
		reference_key (str): Reference sentences are passed to :func:`forward` by ``data[reference_key]``.
			Default: ``reference``.
		gen_key (str): Sentences generated by model are passed to :func:.forward by
			``data[gen_prob_key]``. Default: ``gen``.
		turn_len_key (str): Turn length are passed to  :func:.forward by
			``data[turn_len_key]``. Default: ``turn_length``.
	'''
	def __init__(self, dataloader, reference_key="reference", \
					gen_key="gen", \
					turn_len_key="turn_length" \
			  ):
		super().__init__()
		self.dataloader = dataloader
		self.reference_key = reference_key
		self.turn_len_key = turn_len_key
		self.gen_key = gen_key
		self.refs = []
		self.hyps = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys.
			data[reference_key] (list or :class:`numpy.array`): Reference sentences.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, max_turn_length, max_sentence_length]`
			data[gen_key] (list or :class:`numpy.array`): 3-d array of int.
				Setences generated by model.
				Contains end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
				The 2nd / 3rd dimension can be jagged.
				Size: `[batch_size, max_turn_length, gen_sentence_length]`.
			data[turn_len_key] (list or :class:`numpy.array`): Length of turns in each sample.
				Size: `[batch_size]`
		'''
		reference = data[self.reference_key]
		length = data[self.turn_len_key]
		gen = data[self.gen_key]
		if len(length) != len(reference) or len(length) != len(gen):
			raise ValueError("Batch num is not matched.")

		for i, turn_length in enumerate(length):
			gen_session = gen[i]
			ref_session = reference[i]
			for j in range(turn_length):
				self.hyps.append(self.dataloader.trim_index(gen_session[j]))
				self.refs.append([self.dataloader.trim_index(ref_session[j])[1:]])

	def close(self):
		'''Return a dict which contains:

			* **bleu**: bleu value.
		'''
		try:
			return {"bleu": \
				corpus_bleu(self.refs, self.hyps, smoothing_function=SmoothingFunction().method7)}
		except ZeroDivisionError as _:
			raise ZeroDivisionError("Bleu smoothing divided by zero. This is a known bug of corpus_bleu, \
				usually caused when there is only one sample and the sample length is 1.")

class SingleTurnDialogRecorder(MetricBase):
	'''A metric-like class for recording generated sentences and references.

	Arguments:
		dataloader (DataLoader): A dataloader for translating index to sentences.
		post_key (str): Dialog post are passed to :func:`forward` by ``data[reference_key]``.
			Default: ``post``.
		resp_key (str): Dialog responses are passed to :func:`forward` by ``data[reference_key]``.
			Default: ``resp``.
		gen_key (str): Sentence generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
	'''
	def __init__(self, dataloader, post_key="post", resp_key="resp", gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.post_key = post_key
		self.resp_key = resp_key
		self.gen_key = gen_key
		self.post_list = []
		self.resp_list = []
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys.
			data[post_key] (list or :class:`numpy.array` of `int`): Dialog post.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, max_sentence_length]`
			data[resp_key] (list or :class:`numpy.array` of `int`): Dialog responses.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, max_sentence_length]`
			data[gen_key] (list or :class:`numpy.array` of `int`): Setences generated by model.
				Contains end token (eg: ``<eos>``)`, but without start token (eg: ``<go>``).
				Size: `[batch_size, gen_sentence_length]`.
		'''
		post = data[self.post_key]
		resp = data[self.resp_key]
		gen = data[self.gen_key]
		if len(post) != len(resp) or len(resp) != len(gen):
			raise ValueError("Batch num is not matched.")
		for i, post_sen in enumerate(post):
			self.post_list.append(self.dataloader.index_to_sen(post_sen[1:]))
			self.resp_list.append(self.dataloader.index_to_sen(resp[i][1:]))
			self.gen_list.append(self.dataloader.index_to_sen(gen[i]))

	def close(self):
		'''Return a dict which contains:

			* **post**: a list of post sentences.
			* **resp**: a list of response sentences.
			* **gen**: a list of generated sentences.
		'''
		return {"post": self.post_list, "resp": self.resp_list, "gen": self.gen_list}

class MultiTurnDialogRecorder(MetricBase):
	'''A metric-like class for recording generated sentences and references.

	Arguments:
		dataloader (DataLoader): A dataloader for translating index to sentences.
		context_key (str): Dialog context are passed to :func:`forward` by ``data[context_key]``.
			Default: ``context``.
		reference_key (str): Dialog reference are passed to :func:`forward` by ``data[reference_key]``.
			Default: ``reference``.
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
		turn_len_key (str): Turn length are passed to  :func:.forward by
			``data[turn_len_key]``. Default: ``turn_length``.
	'''
	def __init__(self, dataloader, context_key="context", reference_key="reference", gen_key="gen", \
			turn_len_key="turn_length"):
		super().__init__()
		self.dataloader = dataloader
		self.context_key = context_key
		self.reference_key = reference_key
		self.gen_key = gen_key
		self.turn_len_key = turn_len_key
		self.context_list = []
		self.reference_list = []
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys.
			data[context_key] (list or :class:`numpy.array` of `int`): Dialog post.
				A 3-d padded array containing id of words.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, context_turn_length, max_sentence_length]`
			data[reference_key] (list or :class:`numpy.array` of `int`): Dialog responses.
				A 3-d padded array containing id of words.
				Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				Size: `[batch_size, max_turn_length, max_sentence_length]`
			data[gen_key] (list or :class:`numpy.array` of `int`): Setences generated by model.
				A 3-d padded array containing id of words.
				Contains  end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
				Size: `[batch_size, max_turn_length, gen_sentence_length]`.
			data[turn_len_key] (list or :class:`numpy.array`): Length of turns in each sample.
				Size: `[batch_size]`
		'''
		context = data[self.context_key]
		reference = data[self.reference_key]
		gen = data[self.gen_key]
		turn_length = data[self.turn_len_key]
		if len(context) != len(reference) or len(context) != len(gen):
			raise ValueError("Batch num is not matched.")
		for i, context_sen in enumerate(context):
			self.context_list.append(self.dataloader.multi_turn_index_to_sen( \
				np.array(context_sen), ignore_first_token=True))
			self.reference_list.append(self.dataloader.multi_turn_index_to_sen( \
				np.array(reference[i]), turn_length=turn_length[i], ignore_first_token=True))
			self.gen_list.append(self.dataloader.multi_turn_index_to_sen( \
				np.array(gen[i]), turn_length=turn_length[i]))
			print(turn_length[i])
			print(len(self.reference_list[-1]))

			if len(self.reference_list[-1]) != len(self.gen_list[-1]):
				raise ValueError("Reference turn num %d != gen turn num %d." % \
						(len(self.reference_list[-1]), len(self.gen_list[-1])))

	def close(self):
		'''Return a dict which contains:

			* **context**: a list of post sentences.
			* **reference**: a list of response sentences.
			* **gen**: a list of generated sentences.
		'''
		return {"context": self.context_list, "reference": self.reference_list, "gen": self.gen_list}

class LanguageGenerationRecorder(MetricBase):
	'''A metric-like class for recorder BLEU.

	Arguments:
		dataloader (DataLoader): A dataloader for translating index to sentences.
		gen_key (str): Sentences generated by model are passed to :func:`forward` by
			``data[gen_key]``. Default: ``gen``.
	'''
	def __init__(self, dataloader, gen_key="gen"):
		super().__init__()
		self.dataloader = dataloader
		self.gen_key = gen_key
		self.gen_list = []

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys.
			data[gen_key] (list or :class:`numpy.array` of `int`): Setences generated by model.
				Contains end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
				Size: `[batch_size, gen_sentence_length]`.
		'''
		gen = data[self.gen_key]
		for sen in gen:
			self.gen_list.append(self.dataloader.index_to_sen(sen))

	def close(self):
		'''Return a dict which contains:

			* **gen**: a list of generated sentences.
		'''
		return {"gen": self.gen_list}

class HashValueRecorder(MetricBase):
	'''A metric-like class for recording hash value metric.
	'''
	def __init__(self, hash_key="hashvalue"):
		super().__init__()
		self._hash_key = hash_key
		self.unordered_hash = None

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains hashvalue.
		'''
		if "hashvalue" in data:
			if self.unordered_hash is None:
				self.unordered_hash = UnorderedSha256()
			self.unordered_hash.update_hash(data["hashvalue"])

	def close(self):
		'''Return a dict which contains the items which all the
			meric components returned.
		'''
		if self.unordered_hash:
			return {self._hash_key: self.unordered_hash.digest()}
		else:
			return {}

class MetricChain(MetricBase):
	'''A metric-like class for stacked metric. You can use this class
	making multiples metric combination like one.

	Examples:
		>>> metric = MetricChain()
		>>> metric.add_metric(BleuCorpusMetric())
		>>> metric.add_metric(SingleDialogRecorder(dataloader))
	'''
	def __init__(self):
		super().__init__()
		self.metric_list = []

	def add_metric(self, metric):
		'''Add metric for processing.

		Arguments:
			metric (MetricBase): a metric class
		'''
		if not isinstance(metric, MetricBase):
			raise TypeError("Metric must be a subclass of MetricBase")
		self.metric_list.append(metric)

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains keys which all the
				metric components need.
		'''
		for metric in self.metric_list:
			metric.forward(data)

	def close(self):
		'''Return a dict which contains the items which all the
			meric components returned.
		'''
		ret_dict = {}
		for metric in self.metric_list:
			ret_dict.update(metric.close())
		return ret_dict
